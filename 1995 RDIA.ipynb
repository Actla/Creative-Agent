{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the paper __1995 StorckHochreiterSchmidhuber - Reinforcement driven information acquisition in nondeterministc environments__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Create the Agent that will explore the MDP!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "# \n",
    "# Author: Luca Celotti, \n",
    "# Supervisor: Jean Rouat,\n",
    "# PhD student at Universitè de Sherbrooke (Canada)\n",
    "# Electrical Engineering Department\n",
    "# funded by CHIST-ERA/FRQNT Québec for IGLU\n",
    "# \n",
    "# work finished the 22⋅02⋅2017 in the context of a course on\n",
    "# Reinforcement Learning\n",
    "#\n",
    "# based on the paper \"Reinforcement driven information acquisition in \n",
    "# nondeterministc environments\" (1995 Storck, Hochreiter, Schmidhuber)\n",
    "# published in Proc. ICANN'95 vol.2\n",
    "#\n",
    "#==================================================================\n",
    "\n",
    "from itertools import product\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RDIA:\n",
    "    def __init__(self, n_states = n_states, n_actions = n_actions):\n",
    "        self.n_s = n_states; self.n_a = n_actions\n",
    "        self.Q = np.zeros((self.n_a,self.n_s))\n",
    "        self.c_ijk = np.zeros((self.n_a, self.n_s, self.n_s))\n",
    "        self.P_experimental = np.zeros((self.n_a, self.n_s, self.n_s))\n",
    "\n",
    "        # start from a random place\n",
    "        self.St = np.random.randint(0, self.n_s)\n",
    "        self.Stplus = np.random.randint(0, self.n_s)\n",
    " \n",
    "    def prob_ijk(self,at,c_ijk):\n",
    "        c_ij = np.sum(c_ijk[at,self.St,:])\n",
    "        p_ijk = c_ijk[at,self.St,:]/c_ij\n",
    "\n",
    "        if c_ij == 0:\n",
    "            p_ijk = c_ijk[at,self.St,:]*0\n",
    "        return p_ijk\n",
    "\n",
    "    def information_gain_D(self,at,c_ijk):\n",
    "        p_ijk_t = self.prob_ijk(at,c_ijk)\n",
    "\n",
    "        c_ijk[at,self.St,self.Stplus] += 1\n",
    "        p_ijt_tplus = self.prob_ijk(at,c_ijk)\n",
    "        return sum(abs(p_ijt_tplus-p_ijk_t))    \n",
    "\n",
    "    def update_Q(self,at,c_ijk):\n",
    "        alpha = .5\n",
    "        gamma = .45\n",
    "        D = self.information_gain_D(at,c_ijk)\n",
    "        \n",
    "        maxQ = max(self.Q[:,self.Stplus])\n",
    "        self.Q[at,self.St] = (1-alpha)*self.Q[at,self.St] + alpha*(D+gamma*maxQ)  \n",
    "    \n",
    "    def reconstruct_P(self):\n",
    "         # reconstruct the final experimental transition matrix    \n",
    "        for i,j in product(np.arange(self.n_a), np.arange(self.n_s)):\n",
    "            c_ij = np.sum(self.c_ijk[i,j,:])\n",
    "            for k in np.arange(self.n_s):\n",
    "                self.P_experimental[i,j,k] = self.c_ijk[i,j,k]/c_ij\n",
    "                if c_ij == 0:\n",
    "                    self.P_experimental[i,j,k] = 0\n",
    "                    \n",
    "    \n",
    "    def learner_life(self, terminal_time = 1000, \n",
    "                     epsilon = .5, transition_M=P):\n",
    "        \n",
    "        count = 0\n",
    "        for t in np.arange(terminal_time):\n",
    "\n",
    "            # 1. pick at\n",
    "            # epsilon greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                count += 1\n",
    "                at = np.random.randint(0, self.n_a)\n",
    "            else:\n",
    "                at = np.argmax(self.Q[:,self.St])\n",
    "\n",
    "            # 2. execute at and figure out where you end up, in which S(t+1)\n",
    "            self.Stplus = np.random.choice(self.n_s, 1, p=P[at,self.St,:])[0] \n",
    "\n",
    "            # 3. update Q value\n",
    "            self.update_Q(at,c_ijk)\n",
    "            self.c_ijk[at,self.St,self.Stplus] += 1\n",
    "            self.St = self.Stplus\n",
    "            \n",
    "        self.reconstruct_P()\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Create the MDP that the Agent will explore!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________value function\n",
      "[[ 1.5408514   1.12557695  0.71262179  0.33257408  0.        ]\n",
      " [ 1.35104173  0.93635475  0.49994657  0.15174813  0.        ]\n",
      " [ 2.06243583  1.62524022  1.13946731  0.59971964  0.        ]\n",
      " [ 2.22742992  1.7902343   1.3044614   0.76471372  0.        ]]\n",
      "\n",
      "________________________optimal policy\n",
      "[[2 2 2 2]\n",
      " [1 1 1 2]\n",
      " [0 0 0 0]\n",
      " [2 2 2 2]]\n",
      "\n",
      "________________________transition matrix\n",
      "[[[ 0.          1.          0.          0.        ]\n",
      "  [ 0.          1.          0.          0.        ]\n",
      "  [ 0.          0.          1.          0.        ]\n",
      "  [ 0.22316471  0.40515529  0.11081943  0.26086057]]\n",
      "\n",
      " [[ 0.          0.38463411  0.61536589  0.        ]\n",
      "  [ 0.34345463  0.          0.65654537  0.        ]\n",
      "  [ 0.65828347  0.34171653  0.          0.        ]\n",
      "  [ 0.          0.          1.          0.        ]]\n",
      "\n",
      " [[ 0.11072487  0.33007622  0.55919891  0.        ]\n",
      "  [ 0.          1.          0.          0.        ]\n",
      "  [ 0.46263903  0.          0.53736097  0.        ]\n",
      "  [ 0.          0.          1.          0.        ]]]\n",
      "\n",
      "________________________reward matrix shape\n",
      "(3, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# generate an MDP\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "n_states = 4; n_actions = 3\n",
    "P, R = mdptoolbox.example.rand(n_states, n_actions)\n",
    "fh = mdptoolbox.mdp.FiniteHorizon(P, R, 0.9, 4)\n",
    "fh.run()\n",
    "\n",
    "print '________________________value function'\n",
    "print fh.V\n",
    "print\n",
    "print '________________________optimal policy'\n",
    "print fh.policy\n",
    "print\n",
    "print '________________________transition matrix'\n",
    "print P\n",
    "print\n",
    "print '________________________reward matrix shape'\n",
    "print R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Run the Agent! Start the exploration!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______original______________\n",
      "\n",
      "[[[ 0.          1.          0.          0.        ]\n",
      "  [ 0.          0.47334267  0.49178996  0.03486738]\n",
      "  [ 0.          0.69933796  0.30066204  0.        ]\n",
      "  [ 0.35021047  0.03046641  0.38850726  0.23081586]]\n",
      "\n",
      " [[ 0.31455356  0.29418361  0.05678908  0.33447376]\n",
      "  [ 0.          0.34927368  0.17393582  0.4767905 ]\n",
      "  [ 0.36538103  0.02244378  0.45029022  0.16188496]\n",
      "  [ 0.          0.          0.          1.        ]]\n",
      "\n",
      " [[ 0.12296407  0.31172657  0.56530935  0.        ]\n",
      "  [ 0.          0.          0.45556413  0.54443587]\n",
      "  [ 0.          0.          1.          0.        ]\n",
      "  [ 0.30654109  0.29942345  0.19841147  0.19562399]]]\n",
      "_____________________________\n",
      "\n",
      "\n",
      "_______reconstruction________\n",
      "\n",
      "[[[ 0.          1.          0.          0.        ]\n",
      "  [ 0.          0.53608247  0.42268041  0.04123711]\n",
      "  [ 0.          0.76699029  0.23300971  0.        ]\n",
      "  [ 0.45121951  0.01219512  0.36585366  0.17073171]]\n",
      "\n",
      " [[ 0.26582278  0.3164557   0.07594937  0.34177215]\n",
      "  [ 0.          0.37037037  0.13888889  0.49074074]\n",
      "  [ 0.40677966  0.01129944  0.43502825  0.14689266]\n",
      "  [ 0.          0.          0.          1.        ]]\n",
      "\n",
      " [[ 0.07843137  0.25490196  0.66666667  0.        ]\n",
      "  [ 0.          0.          0.52857143  0.47142857]\n",
      "  [ 0.          0.          1.          0.        ]\n",
      "  [ 0.27956989  0.35483871  0.16129032  0.20430108]]]\n",
      "_____________________________\n"
     ]
    }
   ],
   "source": [
    "MyAgent = RDIA(n_states = n_states, n_actions = n_actions)\n",
    "MyAgent.learner_life(transition_M=P)\n",
    "\n",
    "\n",
    "# compare if the algorithm was successgul\n",
    "print '_______original______________'\n",
    "print\n",
    "print P\n",
    "print '_____________________________'\n",
    "print\n",
    "print\n",
    "print '_______reconstruction________'\n",
    "print\n",
    "print MyAgent.P_experimental\n",
    "print '_____________________________'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
